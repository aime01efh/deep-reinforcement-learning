{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tennis with MADDPG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project we have created two tennis-playing agents that are trained via Multi-Agent Deep Deterministic Policy (MADDPG). The implementation is derived from the solution to a previous Udacity Deep Reinforcement Learning Nanodegree exercise in which a MADDPG is used to train agents to play the Physical Deception problem. See https://arxiv.org/pdf/1706.02275.pdf for the paper introducing MADDPG."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1.  If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01.  Thus, the goal of each agent is to keep the ball in play."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project the agent's learning algorithm is Multi-Agent Deep Deterministic Policy (MADDPG). The MADDPG meta-agent manages multiple DDPG agents, each with two actor networks (local and target) and two critic networks (local and target). The full observation set (plus actions from all agents) is provided to all critic networks for training, but each agent's actor receives only the observations pertaining to that agent.\n",
    "\n",
    "In MADDPG, training is performed by running episodes and storing each step in a replay buffer. The training then iterates over batches of samples randomly drawn from the replay buffer, optimizing both actor and critic networks. \n",
    "\n",
    "A simple neural network architecture is used for both actors and critics. For the actors, the NNs consist of:\n",
    "- two fully-connected hidden layers, of 256 and 128 units, with ReLU activation and dropout\n",
    "- a fully-connected output layer of size corresponding to the number of actions (in this case, two: movement and jumping) with tanh activation which limits its output to the action space\n",
    "\n",
    "For the critics, the NNs consist of:\n",
    "- two fully-connected hidden layers, of 256 and 512 units, with ReLU activation and dropout\n",
    "- a fully-connected output layer with a single unit (and no activation function) estimating the Q of the input values\n",
    "\n",
    "The weights for both actor and critic networks (local copy) are updated using Adam optimizers, and the target instances of each are updated from the local copies via soft updates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameter values used were:\n",
    "\n",
    "```python\n",
    "hidden_in_actor = 256\n",
    "hidden_out_actor = 128\n",
    "hidden_in_critic = 256\n",
    "hidden_out_critic = 512\n",
    "lr_actor = 1e-5\n",
    "lr_critic = 1e-5\n",
    "dropout = 0.1\n",
    "ou_sigma = 0.3\n",
    "\n",
    "batchsize = 512\n",
    "episode_length = 512\n",
    "update_step_interval = 1\n",
    "update_iterations = 5\n",
    "discount_factor = 0.98\n",
    "tau = 1e-2\n",
    "initial_noise_scale = 3.0\n",
    "min_noise_scale = 0.1\n",
    "episode_noise_end = 300\n",
    "replay_buffer_len = 1_000_000\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most challenging part of the project was getting the update cycle correct.  The final pseudocode follows.\n",
    "\n",
    "```python\n",
    "# At each update, run for each agent:\n",
    "\n",
    "# -- UPDATE THE CRITIC NETWORK --\n",
    "# for each replay buffer sample (vectorized across the batch):\n",
    "#   for each agent:\n",
    "#     use target actor NN and agent's next_obs to get agent's target_actions\n",
    "#   concat next_obs_full and all agent target_actions as critic input\n",
    "#   use this critic input with agent's target critic to get Qnext\n",
    "#   y = sample_reward[agent_number] + (discount * Qnext)\n",
    "#\n",
    "#   concat obs_full and all agent sample actions as critic input\n",
    "#   use this critic input with agent's local critic to get Q\n",
    "#   optimize MSE loss between Q and y, updating agent's local critic\n",
    "\n",
    "# -- UPDATE THE AGENT'S ACTOR NETWORK USING POLICY GRADIENT --\n",
    "# for each replay buffer sample (vectorized across the batch):\n",
    "#   use local actor NN and agent's obs to get this agent's new actions\n",
    "#   concat obs_full and all agent actions as critic input (for this\n",
    "#     agent, replace sampled actions with the new actions)\n",
    "#   use this critic input with agent's local critic to get Q\n",
    "#   negatize Q to use as loss function to optimizer of agent's local actor NN\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This update is repeated 5 times at each step of the episode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### networkforall.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`networkforall.py` has the PyTorch deep neural network model that implements both the actor and critic neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ddpg_agent.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ddpg_agent.py` provides the per-agent logic around the neural network. It creates four instances of the NN from networkforall.py: two actor networks (local and target), and two critic networks (local and target). Its interface methods include:\n",
    "- act(): given observations for this agent, return the selected action based on the local policy, optionally with OU noise added\n",
    "- target_act(): given observations for this agent, return the selected action based on the target policy, optionally with OU noise added"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### maddpg_agent.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`maddpg_agent.py`'s MADDPG_Agent class manages the MADDPG algorithm. It keeps an instance of DDPGAgent for each agent. Methods include\n",
    "- act(): return the results of each agent's act()\n",
    "- target_act(): return the results of each agent's target_act\n",
    "- update(): update actor and critic networks of all agents (local networks, not target networks)\n",
    "- update_targets(): soft-update actor and critic target networks of all agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### maddpg_train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`maddpg_train.py` manages the MADDPG training. It runs the desired number of training episodes (stopping early if the goal is reached), as follows:\n",
    "\n",
    "Execute an entire episode, and at each step:\n",
    "- Store the step in the replay buffer\n",
    "- Draw a random sample from the replay buffer\n",
    "- Update the MADDPG agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run_random.py, run_test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the primary interactive method for working with this algorithm is the `Tennis.ipynb` notebook, `run_random.py` and run_test.py allows command-line based runs of the algorithm, either using using randomly chosen hyperparameters or a specific set of hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Plot of Training Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment solved in 2122 episodes!\n"
     ]
    }
   ],
   "source": [
    "from maddpg_train import GOAL_WINDOW_LEN\n",
    "with open('score_history.txt') as fp:\n",
    "    mean_rewards = [float(x) for x in fp.read().splitlines()]\n",
    "print('Environment solved in {:d} episodes!'.format(len(mean_rewards) - GOAL_WINDOW_LEN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(mean_rewards)), mean_rewards)\n",
    "plt.ylabel('Mean episode score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideas for Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other areas in which to search for improvement include:\n",
    "- better optimizing the hyperparameters and neural network architecture\n",
    "- prioritized experience replay in order to learn more efficiently\n",
    "- parameter noise (https://openai.com/blog/better-exploration-with-parameter-noise/) sounds like an interesting topic to investigate\n",
    "- the MADDPG paper mentions the use of entropy regularization which could be explored\n",
    "- the MADDPG paper describes an option for policy ensembles in which agents have multiple subpolicies to help with environment non-stationarity; the paper suggests this is most helpful in competitive environments, which this is not, but it would still be interesting to try"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpful Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the Udacity coursework and help forum, and the [MADDPG paper from Lowe et. al.](https://arxiv.org/abs/1706.02275), the blog post at https://towardsdatascience.com/training-two-agents-to-play-tennis-8285ebfaec5f was helpful while trying to identify issues with my project after it was written but before it was working. In addition, during that time it was also helpful to review the github repo of the blog post's author at https://github.com/tommytracey/DeepRL-P3-Collaboration-Competition in order to gain insights into how my project was going astray as well as to better understand some pytorch usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

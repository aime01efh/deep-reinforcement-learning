{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tennis with MADDPG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project we have created two tennis-playing agents that are trained via Multi-Agent Deep Deterministic Policy (MADDPG). The implementation is derived from the solution to a previous Udacity Deep Reinforcement Learning Nanodegree exercise in which a MADDPG is used to train agents to play the Physical Deception problem. See https://arxiv.org/pdf/1706.02275.pdf for the paper introducing MADDPG."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1.  If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01.  Thus, the goal of each agent is to keep the ball in play."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project the agent's learning algorithm is Multi-Agent Deep Deterministic Policy (MADDPG). The MADDPG meta-agent manages multiple DDPG agents, each with two actor networks (local and target) and two critic networks (local and target). The full observation set (plus actions from all agents) is provided to all critic networks for training, but each agent's actor receives only the observations pertaining to that agent.\n",
    "\n",
    "In MADDPG, training is performed by running episodes and storing each step in a replay buffer. The training then iterates over batches of samples randomly drawn from the replay buffer, optimizing both actor and critic networks. \n",
    "\n",
    "A simple neural network architecture is used for both actors and critics. For the actors, the NNs consist of:\n",
    "- two fully-connected hidden layers with ReLU activation and dropout (three hidden layers were also tried)\n",
    "- a fully-connected output layer of size corresponding to the number of actions (in this case, two: movement and jumping) with tanh activation which limits its output to the action space\n",
    "\n",
    "For the critics, the NNs consist of:\n",
    "- four fully-connected hidden layers with ReLU activation and dropout (two and three hidden layers were also tried)\n",
    "- a fully-connected output layer with a single unit (and no activation function) estimating the Q of the input values\n",
    "\n",
    "The weights for both actor and critic networks (local copy) are updated using Adam optimizers, and the target instances of each are updated from the local copies via soft updates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameter values used were:\n",
    "\n",
    "```python\n",
    "hidden_in_actor = 256\n",
    "hidden_out_actor = 128\n",
    "hidden_in_critic = 256\n",
    "hidden_out_critic = 128\n",
    "lr_actor = 5e-4\n",
    "lr_critic = 1e-3\n",
    "dropout = 0.3\n",
    "\n",
    "batchsize = 512  # Also tried 1024\n",
    "maddpg_train.REPLAY_BUFFER_LEN = 1_000_000\n",
    "episode_length = 200\n",
    "episodes_per_update = 8\n",
    "discount_factor = 0.99  # gamma\n",
    "tau = 1e-3\n",
    "ou_noise = 4.0\n",
    "noise_reduction = 0.9999\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### networkforall.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`networkforall.py` has the PyTorch deep neural network model that implements both the actor and critic neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ddpg_agent.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ddpg_agent.py` provides the per-agent logic around the neural network. It creates four instances of the NN from networkforall.py: two actor networks (local and target), and two critic networks (local and target). Its interface methods include:\n",
    "- act(): given observations for this agent, return the selected action based on the local policy, optionally with OU noise added\n",
    "- target_act(): given observations for this agent, return the selected action based on the target policy, optionally with OU noise added"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### maddpg_agent.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`maddpg_agent.py`'s MADDPG_Agent class manages the MADDPG algorithm. It keeps an instance of DDPGAgent for each agent. Methods include\n",
    "- act(): return the results of each agent's act()\n",
    "- target_act(): return the results of each agent's target_act\n",
    "- update(): update actor and critic networks of all agents (local networks, not target networks)\n",
    "- update_targets(): soft-update actor and critic target networks of all agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### maddpg_train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`maddpg_train.py` manages the MADDPG training. It runs the desired number of training episodes (stopping early if the goal is reached):\n",
    "- Execute an entire episode, storing each step in the replay buffer\n",
    "- Draw a random sample from the replay buffer\n",
    "- Update the MADDPG agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the primary interactive method for working with this algorithm is the `Tennis.ipynb` notebook, `run.py` allows repeated runs of the algorithm using randomly chosen hyperparameters. This was used to try dozens of hyperparameter combinations looking for a set that would provide better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Plot of Training Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment solved in 9900 episodes!\n"
     ]
    }
   ],
   "source": [
    "from maddpg_train import GOAL_WINDOW_LEN\n",
    "with open('score_history.txt') as fp:\n",
    "    mean_rewards = [float(x) for x in fp.read().splitlines()]\n",
    "print('Environment solved in {:d} episodes!'.format(len(mean_rewards) - GOAL_WINDOW_LEN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not yet solved..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(mean_rewards)), mean_rewards)\n",
    "plt.ylabel('Mean episode score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorboard graphs showing per-agent losses and per-agent moving-average rewards:\n",
    "\n",
    "![](tensorboard2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideas for Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preferred first action for future work would be to get this to actually learn. I have already tried many combinations of hyperparameters, changes to weight initialization, batch normalization, both SmoothL1Loss and MSELoss, dropout, a varied number of hidden actor and critic network layers, providing all observations to all agent actors instead of just that agent's observations, and critic gradient clipping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the project shows evidence of the ability to learn, other areas in which to search for improvement include:\n",
    "- properly optimizing the hyperparameters and neural network architecture\n",
    "- prioritized experience replay in order to learn more efficiently\n",
    "- the MADDPG paper mentions the use of entropy regularization which could be explored\n",
    "- the MADDPG paper describes an option for policy ensembles in which agents have multiple subpolicies to help with environment non-stationarity; the paper suggests this is most helpful in competitive environments, which this is not, but it would still be interesting to try"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

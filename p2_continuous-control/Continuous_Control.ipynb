{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the second project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Reacher.app\"`\n",
    "- **Windows** (x86): `\"path/to/Reacher_Windows_x86/Reacher.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Reacher_Windows_x86_64/Reacher.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Reacher_Linux/Reacher.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Reacher_Linux/Reacher.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Reacher.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Reacher.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UnityEnvironment(file_name='Reacher_Linux_20agents/Reacher.x86_64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, a double-jointed arm can move to target locations. A reward of `+0.1` is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "The observation space consists of `33` variables corresponding to position, rotation, velocity, and angular velocities of the arm.  Each action is a vector with four numbers, corresponding to torque applicable to two joints.  Every entry in the action vector must be a number between `-1` and `1`.\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agent's performance, if it selects an action at random with each time step.  A window should pop up that allows you to observe the agent, as it moves through the environment.  \n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agent is able to use its experience to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "while True:\n",
    "    actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from unityagents import UnityEnvironment\n",
    "import ppo\n",
    "import ppo_agent\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_size -> 5.0\n",
      "\t\tgoal_speed -> 1.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"Reacher_Linux_20agents/Reacher.x86_64\")\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ppo_agent.Agent(state_size=len(env_info.vector_observations[0]),\n",
    "                        action_size=brain.vector_action_space_size,\n",
    "                        hidden_sizes=[256, 512],\n",
    "                        seed=237)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L = -0.014573533\n",
      "L = -0.01710531\n",
      "L = -0.02775851\n",
      "L = -0.024357304\n",
      "L = -0.029425008\n",
      "L = -0.031497076\n",
      "L = -0.031365786\n",
      "L = -0.033109415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1000 [00:33<9:12:28, 33.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1, score: 0.198500, window mean: 0.198500\n",
      "L = -0.019400699\n",
      "L = -0.023500593\n",
      "L = -0.026777465\n",
      "L = -0.029274926\n",
      "L = -0.030704768\n",
      "L = -0.031198978\n",
      "L = -0.0315952\n",
      "L = -0.032310173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/1000 [01:05<9:07:39, 32.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 2, score: 0.100500, window mean: 0.149500\n",
      "L = -0.021679627\n",
      "L = -0.02827236\n",
      "L = -0.029426565\n",
      "L = -0.02986296\n",
      "L = -0.030906448\n",
      "L = -0.032244403\n",
      "L = -0.033045433\n",
      "L = -0.032866586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/1000 [01:37<9:04:45, 32.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 3, score: 0.094000, window mean: 0.131000\n",
      "L = -0.022916468\n",
      "L = -0.027612738\n",
      "L = -0.030836811\n",
      "L = -0.031832155\n",
      "L = -0.03240755\n",
      "L = -0.0331399\n",
      "L = -0.03338923\n",
      "L = -0.033716142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/1000 [02:10<9:03:07, 32.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 4, score: 0.091000, window mean: 0.121000\n",
      "L = -0.022516206\n",
      "L = -0.025913771\n",
      "L = -0.032789275\n",
      "L = -0.033811923\n",
      "L = -0.036933366\n",
      "L = -0.036982674\n",
      "L = -0.03772671\n",
      "L = -0.040427763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/1000 [02:43<9:01:45, 32.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 5, score: 0.371500, window mean: 0.171100\n",
      "L = -0.022600193\n",
      "L = -0.027692461\n",
      "L = -0.026619911\n",
      "L = -0.029584441\n",
      "L = -0.03366631\n",
      "L = -0.036246385\n",
      "L = -0.036509316\n",
      "L = -0.03615027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 6/1000 [03:15<9:01:25, 32.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 6, score: 0.238000, window mean: 0.182250\n",
      "L = -0.022595484\n",
      "L = -0.031962477\n",
      "L = -0.031093262\n",
      "L = -0.032708034\n",
      "L = -0.035715133\n",
      "L = -0.036438454\n",
      "L = -0.038193163\n",
      "L = -0.04009487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 7/1000 [03:48<9:00:25, 32.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 7, score: 0.420000, window mean: 0.216214\n",
      "L = -0.022611247\n",
      "L = -0.029964166\n",
      "L = -0.027716864\n",
      "L = -0.029516675\n",
      "L = -0.031194085\n",
      "L = -0.032626744\n",
      "L = -0.03373161\n",
      "L = -0.035444837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 8/1000 [04:20<8:59:36, 32.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 8, score: 0.264000, window mean: 0.222187\n",
      "L = -0.022608513\n",
      "L = -0.029818423\n",
      "L = -0.030741995\n",
      "L = -0.031662602\n",
      "L = -0.032805998\n",
      "L = -0.034331948\n",
      "L = -0.034968514\n",
      "L = -0.03617038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 9/1000 [04:53<8:58:42, 32.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 9, score: 0.293000, window mean: 0.230056\n",
      "L = -0.022379754\n",
      "L = -0.028635727\n",
      "L = -0.027388481\n",
      "L = -0.027713848\n",
      "L = -0.029887183\n",
      "L = -0.031870574\n",
      "L = -0.032988895\n",
      "L = -0.03295123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 10/1000 [05:29<9:14:07, 33.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 10, score: 0.318000, window mean: 0.238850\n",
      "L = -0.022162251\n",
      "L = -0.018038679\n",
      "L = -0.023605451\n",
      "L = -0.026304064\n",
      "L = -0.024303155\n",
      "L = -0.026513606\n",
      "L = -0.030874733\n",
      "L = -0.028405564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 11/1000 [06:02<9:08:56, 33.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 11, score: 0.340500, window mean: 0.248091\n",
      "L = -0.022170682\n",
      "L = -0.02117964\n",
      "L = -0.023692837\n",
      "L = -0.02489183\n",
      "L = -0.027390864\n",
      "L = -0.027429953\n",
      "L = -0.030617328\n",
      "L = -0.02947809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 12/1000 [06:34<9:06:05, 33.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 12, score: 0.373000, window mean: 0.258500\n",
      "L = -0.022178594\n",
      "L = -0.01114295\n",
      "L = -0.023685463\n",
      "L = -0.017301213\n",
      "L = -0.024389192\n",
      "L = -0.027770156\n",
      "L = -0.025035826\n",
      "L = -0.029994084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 13/1000 [07:07<9:02:38, 32.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 13, score: 0.274000, window mean: 0.259692\n",
      "L = -0.022068916\n",
      "L = -0.025277672\n",
      "L = -0.026986262\n",
      "L = -0.028202035\n",
      "L = -0.028369438\n",
      "L = -0.029506207\n",
      "L = -0.030427488\n",
      "L = -0.031029526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 14/1000 [07:40<9:00:21, 32.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 14, score: 0.279000, window mean: 0.261071\n",
      "L = -0.02194202\n",
      "L = -0.0046856245\n",
      "L = -0.024820773\n",
      "L = -0.010533242\n",
      "L = -0.021839283\n",
      "L = -0.025219878\n",
      "L = -0.018601708\n",
      "L = -0.028014777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 15/1000 [08:12<8:59:15, 32.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 15, score: 0.322500, window mean: 0.265167\n",
      "L = -0.021828445\n",
      "L = -0.0257586\n",
      "L = -0.025280515\n",
      "L = -0.02781327\n",
      "L = -0.02858126\n",
      "L = -0.029681241\n",
      "L = -0.030838748\n",
      "L = -0.031115506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 16/1000 [08:45<8:57:56, 32.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 16, score: 0.278500, window mean: 0.266000\n",
      "L = -0.021832367\n",
      "L = -0.016378468\n",
      "L = -0.017483473\n",
      "L = -0.021956904\n",
      "L = -0.019579954\n",
      "L = -0.02069379\n",
      "L = -0.024524773\n",
      "L = -0.024687728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 17/1000 [09:18<8:56:23, 32.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 17, score: 0.195000, window mean: 0.261824\n",
      "L = -0.02159895\n",
      "L = 0.029835083\n",
      "L = -0.015844552\n",
      "L = -0.0025692198\n",
      "L = 0.003684335\n",
      "L = -0.0032457835\n",
      "L = -0.014773009\n",
      "L = -0.014833362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 18/1000 [09:50<8:54:51, 32.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 18, score: 0.258500, window mean: 0.261639\n",
      "L = -0.021546539\n",
      "L = -0.0038396714\n",
      "L = -0.009363254\n",
      "L = -0.019643541\n",
      "L = -0.015300021\n",
      "L = -0.025273576\n",
      "L = -0.022350848\n",
      "L = -0.02207638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 19/1000 [10:23<8:54:57, 32.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 19, score: 0.251000, window mean: 0.261079\n",
      "L = -0.021317275\n",
      "L = -0.020063983\n",
      "L = -0.024414262\n",
      "L = -0.021782475\n",
      "L = -0.025551237\n",
      "L = -0.024001546\n",
      "L = -0.027004762\n",
      "L = -0.026093896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 20/1000 [10:56<8:53:21, 32.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 20, score: 0.323000, window mean: 0.264175\n",
      "L = -0.021300515\n",
      "L = -0.013974147\n",
      "L = -0.020880915\n",
      "L = -0.019231966\n",
      "L = -0.023486562\n",
      "L = -0.02476889\n",
      "L = -0.022014463\n",
      "L = -0.023700492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 21/1000 [11:28<8:53:53, 32.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 21, score: 0.322500, window mean: 0.266952\n",
      "L = -0.021329468\n",
      "L = -0.010332599\n",
      "L = -0.018642513\n",
      "L = -0.019241525\n",
      "L = -0.016381541\n",
      "L = -0.026051993\n",
      "L = -0.023074063\n",
      "L = -0.021550652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 22/1000 [12:01<8:52:11, 32.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 22, score: 0.241000, window mean: 0.265773\n",
      "L = -0.021275885\n",
      "L = -0.016692268\n",
      "L = -0.016402286\n",
      "L = -0.021495204\n",
      "L = -0.019549811\n",
      "L = -0.020766659\n",
      "L = -0.024385918\n",
      "L = -0.02452491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 23/1000 [12:33<8:50:33, 32.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 23, score: 0.182500, window mean: 0.262152\n",
      "L = -0.021082666\n",
      "L = 0.009629797\n",
      "L = 0.0010667711\n",
      "L = -0.022038521\n",
      "L = -0.0059885266\n",
      "L = 0.0008487309\n",
      "L = -0.013013344\n",
      "L = -0.019245405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 24/1000 [13:06<8:50:26, 32.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 24, score: 0.224000, window mean: 0.260562\n",
      "L = -0.020866321\n",
      "L = -0.0027404143\n",
      "L = -0.004272875\n",
      "L = -0.012563822\n",
      "L = -0.010951336\n",
      "L = -0.008941685\n",
      "L = -0.013011622\n",
      "L = -0.018284148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▎         | 25/1000 [13:39<8:50:05, 32.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 25, score: 0.202500, window mean: 0.258240\n",
      "L = -0.02056763\n",
      "L = 0.01918385\n",
      "L = -0.0049849562\n",
      "L = -0.015746841\n",
      "L = -0.0059578205\n",
      "L = -0.002423194\n",
      "L = -0.0074706296\n",
      "L = -0.012584627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 26/1000 [14:11<8:49:06, 32.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 26, score: 0.267500, window mean: 0.258596\n",
      "L = -0.020568665\n",
      "L = 0.016059922\n",
      "L = 2.2263815e-05\n",
      "L = -0.018987712\n",
      "L = -0.004938846\n",
      "L = -0.0029652382\n",
      "L = -0.015252456\n",
      "L = -0.01651249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 27/1000 [14:44<8:48:46, 32.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 27, score: 0.285500, window mean: 0.259593\n",
      "L = -0.020343635\n",
      "L = -0.0025263312\n",
      "L = -0.010282457\n",
      "L = -0.01901517\n",
      "L = -0.010935216\n",
      "L = -0.01850937\n",
      "L = -0.021703538\n",
      "L = -0.017137472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 28/1000 [15:16<8:47:57, 32.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 28, score: 0.382000, window mean: 0.263964\n",
      "L = -0.020155622\n",
      "L = -0.0062330198\n",
      "L = -0.0037558386\n",
      "L = -0.015958594\n",
      "L = -0.015058049\n",
      "L = -0.013511754\n",
      "L = -0.020630863\n",
      "L = -0.019225925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 29/1000 [15:49<8:48:36, 32.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 29, score: 0.317000, window mean: 0.265793\n",
      "L = -0.020013265\n",
      "L = -0.01221982\n",
      "L = -0.01143809\n",
      "L = -0.02063321\n",
      "L = -0.018646754\n",
      "L = -0.019215256\n",
      "L = -0.021724686\n",
      "L = -0.023601016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 30/1000 [16:22<8:48:23, 32.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 30, score: 0.244000, window mean: 0.265067\n",
      "L = -0.019754747\n",
      "L = 0.0066351127\n",
      "L = -0.0127518345\n",
      "L = -0.013881691\n",
      "L = -0.006899439\n",
      "L = -0.01619214\n",
      "L = -0.02021275\n",
      "L = -0.017632794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 31/1000 [16:55<8:47:43, 32.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 31, score: 0.421500, window mean: 0.270113\n",
      "L = -0.019654142\n",
      "L = -0.01938715\n",
      "L = -0.019583188\n",
      "L = -0.022336366\n",
      "L = -0.022692567\n",
      "L = -0.023540411\n",
      "L = -0.024179932\n",
      "L = -0.024653846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 32/1000 [17:27<8:46:00, 32.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 32, score: 0.210500, window mean: 0.268250\n",
      "L = -0.019304315\n",
      "L = -0.01417062\n",
      "L = -0.020602351\n",
      "L = -0.017319107\n",
      "L = -0.02134589\n",
      "L = -0.02278906\n",
      "L = -0.021540826\n",
      "L = -0.025117118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 33/1000 [17:59<8:44:35, 32.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 33, score: 0.398000, window mean: 0.272182\n",
      "L = -0.01942785\n",
      "L = -0.005141509\n",
      "L = -0.015503193\n",
      "L = -0.014879511\n",
      "L = -0.01620475\n",
      "L = -0.02364555\n",
      "L = -0.019399386\n",
      "L = -0.022293847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 34/1000 [18:32<8:44:46, 32.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 34, score: 0.288000, window mean: 0.272647\n",
      "L = -0.019278271\n",
      "L = -0.015540175\n",
      "L = -0.016582016\n",
      "L = -0.021222087\n",
      "L = -0.01989924\n",
      "L = -0.023148162\n",
      "L = -0.023849277\n",
      "L = -0.023698252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 35/1000 [19:05<8:43:53, 32.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 35, score: 0.472500, window mean: 0.278357\n",
      "L = -0.01911692\n",
      "L = -0.016363118\n",
      "L = -0.020646192\n",
      "L = -0.018670026\n",
      "L = -0.022025624\n",
      "L = -0.022683736\n",
      "L = -0.023125883\n",
      "L = -0.024713516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 36/1000 [19:37<8:42:45, 32.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 36, score: 0.475500, window mean: 0.283833\n",
      "L = -0.019147404\n",
      "L = -0.012470169\n",
      "L = -0.013936119\n",
      "L = -0.019535786\n",
      "L = -0.015691735\n",
      "L = -0.018824862\n",
      "L = -0.023304855\n",
      "L = -0.022324983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 37/1000 [20:10<8:43:17, 32.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 37, score: 0.331000, window mean: 0.285108\n",
      "L = -0.019008853\n",
      "L = 0.0047358763\n",
      "L = -0.017778369\n",
      "L = -0.012387106\n",
      "L = -0.008195881\n",
      "L = -0.019204054\n",
      "L = -0.019866979\n",
      "L = -0.014580632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 38/1000 [20:42<8:42:48, 32.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 38, score: 0.318500, window mean: 0.285987\n",
      "L = -0.019055832\n",
      "L = -0.010296174\n",
      "L = -0.011937068\n",
      "L = -0.021022828\n",
      "L = -0.011763708\n",
      "L = -0.016380768\n",
      "L = -0.022748848\n",
      "L = -0.01580833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 39/1000 [21:15<8:42:46, 32.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 39, score: 0.277500, window mean: 0.285769\n",
      "L = -0.018613974\n",
      "L = -0.005147609\n",
      "L = -0.021045964\n",
      "L = -0.009913581\n",
      "L = -0.018130112\n",
      "L = -0.01939469\n",
      "L = -0.015469567\n",
      "L = -0.024335947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 40/1000 [21:48<8:41:48, 32.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 40, score: 0.519500, window mean: 0.291612\n",
      "L = -0.018998474\n",
      "L = -0.020206368\n",
      "L = -0.019225428\n",
      "L = -0.019692056\n",
      "L = -0.02140857\n",
      "L = -0.022317423\n",
      "L = -0.022562644\n",
      "L = -0.022589898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 41/1000 [22:20<8:41:19, 32.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 41, score: 0.178500, window mean: 0.288854\n",
      "L = -0.018727602\n",
      "L = -0.004362431\n",
      "L = -0.013971023\n",
      "L = -0.017146932\n",
      "L = -0.010087371\n",
      "L = -0.019699313\n",
      "L = -0.02261283\n",
      "L = -0.017537314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 42/1000 [22:53<8:40:37, 32.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 42, score: 0.382500, window mean: 0.291083\n",
      "L = -0.018668057\n",
      "L = -0.0130565595\n",
      "L = -0.012829027\n",
      "L = -0.018307181\n",
      "L = -0.015632778\n",
      "L = -0.020716561\n",
      "L = -0.023685409\n",
      "L = -0.02175234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 43/1000 [23:26<8:40:20, 32.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 43, score: 0.342000, window mean: 0.292267\n",
      "L = -0.018627195\n",
      "L = -0.0161376\n",
      "L = -0.01672758\n",
      "L = -0.020276513\n",
      "L = -0.018950894\n",
      "L = -0.01864834\n",
      "L = -0.020995425\n",
      "L = -0.023006015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 44/1000 [23:58<8:39:39, 32.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 44, score: 0.276500, window mean: 0.291909\n",
      "L = -0.018552156\n",
      "L = -0.007054372\n",
      "L = -0.009629577\n",
      "L = -0.021456368\n",
      "L = -0.013814562\n",
      "L = -0.014970996\n",
      "L = -0.020442152\n",
      "L = -0.022401892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 45/1000 [24:31<8:38:59, 32.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 45, score: 0.273000, window mean: 0.291489\n",
      "L = -0.018357847\n",
      "L = -0.005483129\n",
      "L = -0.0074534942\n",
      "L = -0.019300403\n",
      "L = -0.013325116\n",
      "L = -0.010478889\n",
      "L = -0.015238212\n",
      "L = -0.020334037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 46/1000 [25:03<8:38:06, 32.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 46, score: 0.259500, window mean: 0.290793\n",
      "L = -0.018173268\n",
      "L = 0.017151615\n",
      "L = -0.017526727\n",
      "L = -0.0072334707\n",
      "L = -0.0041520204\n",
      "L = -0.013877673\n",
      "L = -0.014181861\n",
      "L = -0.012351013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 47/1000 [25:36<8:37:01, 32.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 47, score: 0.310500, window mean: 0.291213\n",
      "L = -0.018104458\n",
      "L = -0.0040786434\n",
      "L = -0.01305648\n",
      "L = -0.016946983\n",
      "L = -0.010980873\n",
      "L = -0.018563764\n",
      "L = -0.020123895\n",
      "L = -0.016966615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 48/1000 [26:09<8:37:26, 32.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 48, score: 0.429500, window mean: 0.294094\n",
      "L = -0.018231984\n",
      "L = -0.013827167\n",
      "L = -0.014823454\n",
      "L = -0.019795464\n",
      "L = -0.018348122\n",
      "L = -0.01645141\n",
      "L = -0.019387199\n",
      "L = -0.022579512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 49/1000 [26:41<8:37:19, 32.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 49, score: 0.235000, window mean: 0.292888\n",
      "L = -0.01806686\n",
      "L = -0.0020399257\n",
      "L = -0.0090037035\n",
      "L = -0.020697124\n",
      "L = -0.010197467\n",
      "L = -0.01718579\n",
      "L = -0.022620983\n",
      "L = -0.018159905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 50/1000 [27:14<8:37:09, 32.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 50, score: 0.411000, window mean: 0.295250\n",
      "L = -0.017816512\n",
      "L = -0.0067065037\n",
      "L = -0.019468036\n",
      "L = -0.013362447\n",
      "L = -0.012993302\n",
      "L = -0.020622905\n",
      "L = -0.018960724\n",
      "L = -0.017597856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 51/1000 [27:47<8:36:59, 32.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 51, score: 0.455000, window mean: 0.298382\n",
      "L = -0.017878857\n",
      "L = -0.01987358\n",
      "L = -0.020723624\n",
      "L = -0.022183929\n",
      "L = -0.022895133\n",
      "L = -0.023665076\n",
      "L = -0.024342401\n",
      "L = -0.024299385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 52/1000 [28:19<8:35:30, 32.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 52, score: 0.253000, window mean: 0.297510\n",
      "L = -0.017795738\n",
      "L = -0.0189332\n",
      "L = -0.019946892\n",
      "L = -0.020252625\n",
      "L = -0.020673752\n",
      "L = -0.02120682\n",
      "L = -0.021584842\n",
      "L = -0.021906488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 53/1000 [28:52<8:34:37, 32.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 53, score: 0.277000, window mean: 0.297123\n",
      "L = -0.017841179\n",
      "L = -0.015540378\n",
      "L = -0.016497046\n",
      "L = -0.020916551\n",
      "L = -0.01799581\n",
      "L = -0.020299137\n",
      "L = -0.023239652\n",
      "L = -0.022348665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 54/1000 [29:24<8:33:31, 32.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 54, score: 0.264000, window mean: 0.296509\n",
      "L = -0.017585386\n",
      "L = -0.011795394\n",
      "L = -0.018491866\n",
      "L = -0.016654553\n",
      "L = -0.016466213\n",
      "L = -0.022385327\n",
      "L = -0.02098573\n",
      "L = -0.022171807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 55/1000 [29:57<8:34:23, 32.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 55, score: 0.362000, window mean: 0.297700\n",
      "L = -0.01737466\n",
      "L = -0.017701378\n",
      "L = -0.01814799\n",
      "L = -0.020334074\n",
      "L = -0.020669248\n",
      "L = -0.021604942\n",
      "L = -0.021988798\n",
      "L = -0.023041671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 56/1000 [30:30<8:33:56, 32.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 56, score: 0.345000, window mean: 0.298545\n",
      "L = -0.01740172\n",
      "L = -0.017827624\n",
      "L = -0.018744094\n",
      "L = -0.021477431\n",
      "L = -0.02135332\n",
      "L = -0.021484174\n",
      "L = -0.02261294\n",
      "L = -0.022944894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 57/1000 [31:02<8:32:37, 32.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 57, score: 0.285500, window mean: 0.298316\n",
      "L = -0.01730001\n",
      "L = -0.0100962985\n",
      "L = -0.0141854165\n",
      "L = -0.01880519\n",
      "L = -0.015350109\n",
      "L = -0.016996803\n",
      "L = -0.020747188\n",
      "L = -0.01936723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 58/1000 [31:35<8:31:30, 32.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 58, score: 0.378000, window mean: 0.299690\n",
      "L = -0.017462946\n",
      "L = -0.012642647\n",
      "L = -0.013071159\n",
      "L = -0.018493589\n",
      "L = -0.016849086\n",
      "L = -0.016294217\n",
      "L = -0.01928807\n",
      "L = -0.020843707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 59/1000 [32:07<8:31:19, 32.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 59, score: 0.231500, window mean: 0.298534\n",
      "L = -0.017253367\n",
      "L = -0.0014862779\n",
      "L = -0.009766927\n",
      "L = -0.016867226\n",
      "L = -0.008672206\n",
      "L = -0.0077417404\n",
      "L = -0.014269378\n",
      "L = -0.016846292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 60/1000 [32:40<8:32:48, 32.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 60, score: 0.338500, window mean: 0.299200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 60/1000 [32:42<8:32:30, 32.71s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-47df6e6ec88d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmean_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mppo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_ppo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/fserv_data/efh/training/udacity/deep-reinforcement-learning/p2_continuous-control/ppo.py\u001b[0m in \u001b[0;36mtrain_ppo\u001b[0;34m(env, agent, num_episodes, epsilon, discount_rate, beta, tmax, num_sgd_epoch, learn_rate, report_every, score_goal, progressbar)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m# Gather trajectories from all parallel agents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mprob_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_trajectories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mtotal_episode_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/fserv_data/efh/training/udacity/deep-reinforcement-learning/p2_continuous-control/utils.py\u001b[0m in \u001b[0;36mcollect_trajectories\u001b[0;34m(env, agent, tmax, nrand)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_observations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# advance the game\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/fserv_data/efh/training/udacity/deep-reinforcement-learning/p2_continuous-control/ppo_agent.py\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0maction_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;31m# Create Normal distributions, sample actions, get log probabilities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/drlnd/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/fserv_data/efh/training/udacity/deep-reinforcement-learning/p2_continuous-control/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mrepresenting\u001b[0m \u001b[0mstatistics\u001b[0m \u001b[0mof\u001b[0m \u001b[0ma\u001b[0m \u001b[0mNormal\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \"\"\"\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mmeans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeans_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/drlnd/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/drlnd/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/drlnd/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m    992\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 994\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mean_rewards = ppo.train_ppo(env, agent, report_every=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save('checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
